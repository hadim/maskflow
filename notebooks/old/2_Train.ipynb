{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import maskflow\n",
    "\n",
    "root_dir = Path(\"/home/hadim/.data/Neural_Network/Maskflow/Shapes\")\n",
    "data_dir = root_dir / \"Data\"\n",
    "log_dir = root_dir / \"Logs\"\n",
    "\n",
    "# Import the configuration associated with this dataset\n",
    "config = maskflow.load_config(root_dir / \"config.yml\")\n",
    "model_basename = config[\"NAME\"]\n",
    "model_dir = log_dir / model_basename\n",
    "saved_model_dir = root_dir / \"SavedModels\" / model_basename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset\n",
    "\n",
    "This step should be adapted to your usecase. The important is to return a tuple of dict with the following signature: `{\"images\": Tensor, \"masks\": Tensor}, {\"class_ids\": Tensor}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadim/local/conda/envs/nn/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "# Get paths\n",
    "image_paths = np.array([str(fname) for fname in sorted(data_dir.glob(\"*_original.tif\"))])\n",
    "mask_paths = np.array([str(fname) for fname in sorted(data_dir.glob(\"*_mask.tif\"))])\n",
    "class_ids_paths = np.array([str(fname) for fname in sorted(data_dir.glob(\"*_class_ids.csv\"))])\n",
    "\n",
    "# Split training and evaluation dataset\n",
    "train_indices, eval_indices = train_test_split(np.arange(0, len(image_paths)), train_size=0.8)\n",
    "\n",
    "# Training dataset\n",
    "image_paths_train = image_paths[train_indices]\n",
    "mask_paths_train = mask_paths[train_indices]\n",
    "class_ids_paths_train = class_ids_paths[train_indices]\n",
    "\n",
    "# Evaluation dataset\n",
    "image_paths_eval = image_paths[eval_indices]\n",
    "mask_paths_eval = mask_paths[eval_indices]\n",
    "class_ids_paths_eval = class_ids_paths[eval_indices]\n",
    "\n",
    "def input_fn(image_paths, mask_paths, class_ids_paths, batch_size, max_objects, shuffle=True, repeat=False):\n",
    "    def _input_fn():\n",
    "        dataset = maskflow.load_dataset_from_paths(image_paths, mask_paths, class_ids_paths,\n",
    "                                                   max_objects=max_objects, shuffle=shuffle)\n",
    "        if repeat:\n",
    "            dataset = dataset.repeat()\n",
    "        \n",
    "        dataset = dataset.prefetch(2 * batch_size)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        images, masks, class_ids = iterator.get_next()\n",
    "        \n",
    "        inputs = {}\n",
    "        inputs[\"images\"] = images\n",
    "        inputs[\"image_height\"] = tf.constant(200, dtype=tf.int32)\n",
    "        inputs[\"image_width\"] = tf.constant(200, dtype=tf.int32)\n",
    "        inputs[\"image_channel\"] = tf.constant(3, dtype=tf.int32)\n",
    "        \n",
    "        outputs = {}\n",
    "        outputs[\"class_ids\"] = class_ids\n",
    "        outputs[\"masks\"] = masks\n",
    "        \n",
    "        return inputs, outputs\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x144 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAyUAAACZCAYAAAAxdJUMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAACS1JREFUeJzt3FuopWUdx/Hf3zSti1I7GUUoEhgdh5DwkJoZ6qQSkV1VdCSiuehAkUEk0UGKTjDQlSlEUBfdaCbW5GFmbDSpQTDsbERIB3PsKq3s6WK9I4vNzHamWWv/9579+cBm1vuuZz373TOvON95nrVqjBEAAIAux3RfAAAAsLmJEgAAoJUoAQAAWokSAACglSgBAABaiRIAAKDVURUlVXVqVe1Yce63/8c8N1fVlunx1qp6uKpqOv5iVb39SV7/tKr6UVXtrqq7qurSuec+WVV3VtWtVXXqdO6Eqvp2Ve2afj1hlbnfUVV3V9XOqvpOVR0/97PfOs39ybnxl1TVnunr4sP9vWD9q6pTqurLhzH+9qp64TKvCQDgcBxVUbJAu5OcMz0+J8nPk7x07njXk7z+P0neN8Y4N8llSb6WJFV1RpILxxjnJLk6yTXT+Hcm+eUY47VJfjUdr3ZtZ48xzkvyxyRvm85fk+TT09wXVtUZVfWUJF9Mcun09aXpHEeRMcafxxgfXXnenzUAsFFsyiipqm9MKw7HVNUtVfWaFUN2Jzl3evzKJN9Icu60KnHKGOMPq80/xvj33JhHk/x3enxBkpumMTunufef//70+MYk51XV8dNKyxlV9byq+mlVnTjG+P0Y4/Fp7L8yC6AkedUYY38s3ZTkvCQvTvLAGOORMcYjSR5IcvrqvztsBFV1zbT6dVtVvX//CmFVXV1V11fVDUneWlWvm1bPbq+qrx5gni9U1R3TXJet+Q8CAJDk2O4LWIJXV9XtTzLmw0luzWzV48djjLtXPH93km9W1XFJRpKdSb6c5L4kP02SqjoryRcOMPdnxhi3zh1/PbPViiQ5OcmDc889Ze78vunxI0meNcZ4rKreneT6JP9I8qEpLDJ9/5ck2Zrk7OnUfGA+kuSUFfM+MfcBrpkNpKq2JnlRZitmo6pOT3Ll3JDHxhhXTFsO709y/hjjLytXTqrqkiQnjTHOr6qnJ9lTVTeNMcZa/SwAAMnRGSU/G2NctP/gQO8pGWM8WlXXZRYLzz/I839N8uYke8cYf6uqUzJbPdk9jdmT2QrHQVXVp5LsG2NcN516OMmJc0MeP8D5Z07HGWP8uqoeSHLyGOMnc/O+MLNYuXKM8eh0ev9qzPwcK7/fE3Ozob0syW1z8fD4iuf33yvPSfL3McZfkmRuhW2/lyc5fy7ij88sWh9a+BWzqVXVtiRvSfLbMcZ7u6+Hzcc9yHrgPlzdZt2+9fwk70ny2SSfP8iw3Uk+nuTO6fjBzP41etc0x1nTlpiVXxdOz2/LbPvUx+bmvCOz93akqs5Ocu/c+a3T463TcarqDUmOS/JQVV0xnXt2ku8l+cAY43dzc987zZnpe+xM8pskp1XVM6rqGUlOS3LYb/xn3bkvyflzxyv/O94fH39LcnJVPSdJqmrluF8k+eEY44IxxgVJXjHGECQs3Bhj+3Sf+Z8wLdyDrAfuw9UdjSslq5r+YnZdZtuh7po+weqNY4ybVgzdleQjSe6aju9M8qbM/kK46kpJVT03s21be5LcNn1w1+vHGPdP7xO5M7P3g7xnesn1mW0X25XkT0neNc3xuSQXZ/a+kR1V9fMkn0jygiRfmeb91hjj2iRXJbm2qp6a5OYxxv3TtVyV5Jbp+1x1gH8tZ4MZY/ygqi6oqj1J/pnkuwcZN6rqg0luqKrHkuzNbOvi/DxnTSslI7N7b9VPlgMAWIayfRwAAOi0KbdvAQAA64coAQAAWokSAACglSgBAABaiRIAAKDVuvhI4Ncds9tHgG0wt/333Oq+hkV72pZt7sMN5p97tx9V96F7cOM52u7BxH24EbkPWQ+O9D60UgIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtDq2+wI2mxsv2nvQ5y7fsWUNr4TNbN892w/63ElnblvDKwEAECVrYrUQOdg4gcKirRYiBxsnUACAtWD71hLdeNHeQw6SA70WFmHfPdsPOUgO9FoAgGUTJeuYMGE9ECYAwLKJkiVZVFAIE47EooJCmAAAyyRKlkBIsB4ICQBgoxAlC7aMIDmS96awOS0jSI7kvSkAAKsRJQAAQCtRAgAAtBIlC7TsLVa2cHEolr3FyhYuAGDRRAkAANBKlAAAAK1ECQAA0EqUAAAArUQJAADQSpQAAACtRMkCXb5jy4aen6PDSWdu29DzAwCbjygBAABaiRIAAKCVKFmwZWyxunzHFlu3OCzL2GJ10pnbbN0CAJZClCyBgGA9EBAAwEYhStY5gcN6IHAAgGUSJUuyiJgQJBypRcSEIAEAlk2ULNGRvBdEkLAoR/JeEEECAKyFY7svYDOYD4wbL9p7SONg0eYDY9892w9pHADAWhAla0x4sB4IDwBgPbF9CwAAaCVKAACAVqIEAABoJUoAAIBWogQAAGglSgAAgFaiBAAAaCVKAACAVqIEAABoJUoAAIBWogQAAGglSgAAgFaiBAAAaFVjjO5rAAAANjErJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAEArUQIAALQSJQAAQCtRAgAAtBIlAABAK1ECAAC0EiUAAECr/wE5ORXh0bc8FAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x144 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x144 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x144 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "example_dataset = input_fn(image_paths, mask_paths, class_ids_paths, batch_size, config[\"MAX_OBJECTS\"], shuffle=True)()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for i in range(2):\n",
    "        features, labels = sess.run(example_dataset)\n",
    "        _ = maskflow.display_top_masks(features[\"images\"], labels[\"masks\"],\n",
    "                                       labels[\"class_ids\"], config[\"CLASS_NAMES\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras.layers as KL\n",
    "import keras.engine as KE\n",
    "\n",
    "############################################################\n",
    "#  Proposal Layer\n",
    "############################################################\n",
    "\n",
    "def apply_box_deltas_graph(boxes, deltas):\n",
    "    \"\"\"Applies the given deltas to the given boxes.\n",
    "    boxes: [N, (y1, x1, y2, x2)] boxes to update\n",
    "    deltas: [N, (dy, dx, log(dh), log(dw))] refinements to apply\n",
    "    \"\"\"\n",
    "    # Convert to y, x, h, w\n",
    "    height = boxes[:, 2] - boxes[:, 0]\n",
    "    width = boxes[:, 3] - boxes[:, 1]\n",
    "    center_y = boxes[:, 0] + 0.5 * height\n",
    "    center_x = boxes[:, 1] + 0.5 * width\n",
    "    \n",
    "    # Apply deltas\n",
    "    center_y += deltas[:, 0] * height\n",
    "    center_x += deltas[:, 1] * width\n",
    "    height *= tf.exp(deltas[:, 2])\n",
    "    width *= tf.exp(deltas[:, 3])\n",
    "    \n",
    "    # Convert back to y1, x1, y2, x2\n",
    "    y1 = center_y - 0.5 * height\n",
    "    x1 = center_x - 0.5 * width\n",
    "    y2 = y1 + height\n",
    "    x2 = x1 + width\n",
    "    result = tf.stack([y1, x1, y2, x2], axis=1, name=\"apply_box_deltas_out\")\n",
    "    return result\n",
    "\n",
    "\n",
    "def clip_boxes_graph(boxes, window):\n",
    "    \"\"\"\n",
    "    boxes: [N, (y1, x1, y2, x2)]\n",
    "    window: [4] in the form y1, x1, y2, x2\n",
    "    \"\"\"\n",
    "    # Split\n",
    "    wy1, wx1, wy2, wx2 = tf.split(window, 4)\n",
    "    y1, x1, y2, x2 = tf.split(boxes, 4, axis=1)\n",
    "    \n",
    "    # Clip\n",
    "    y1 = tf.maximum(tf.minimum(y1, wy2), wy1)\n",
    "    x1 = tf.maximum(tf.minimum(x1, wx2), wx1)\n",
    "    y2 = tf.maximum(tf.minimum(y2, wy2), wy1)\n",
    "    x2 = tf.maximum(tf.minimum(x2, wx2), wx1)\n",
    "    clipped = tf.concat([y1, x1, y2, x2], axis=1, name=\"clipped_boxes\")\n",
    "    clipped.set_shape((clipped.shape[0], 4))\n",
    "    return clipped\n",
    "\n",
    "\n",
    "class ProposalLayer2(KE.Layer):\n",
    "    \"\"\"Receives anchor scores and selects a subset to pass as proposals\n",
    "    to the second stage. Filtering is done based on anchor scores and\n",
    "    non-max suppression to remove overlaps. It also applies bounding\n",
    "    box refinement deltas to anchors.\n",
    "    Inputs:\n",
    "        rpn_probs: [batch, anchors, (bg prob, fg prob)]\n",
    "        rpn_bbox: [batch, anchors, (dy, dx, log(dh), log(dw))]\n",
    "        anchors: [batch, (y1, x1, y2, x2)] anchors in normalized coordinates\n",
    "    Returns:\n",
    "        Proposals in normalized coordinates [batch, rois, (y1, x1, y2, x2)]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, proposal_count, nms_threshold, rpn_bbox_std_dev, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.rpn_bbox_std_dev = rpn_bbox_std_dev\n",
    "        self.proposal_count = proposal_count\n",
    "        self.nms_threshold = nms_threshold\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Box Scores. Use the foreground class confidence. [Batch, num_rois, 1]\n",
    "        scores = tf.cast(inputs[0][:, :, 1], dtype=tf.float32)\n",
    "        \n",
    "        # Box deltas [batch, num_rois, 4]\n",
    "        deltas = inputs[1]\n",
    "        deltas = tf.cast(deltas * np.reshape(self.rpn_bbox_std_dev, [1, 1, 4]), dtype=tf.float32)\n",
    "        \n",
    "        # Anchors\n",
    "        anchors = tf.cast(inputs[2], dtype=tf.float32)\n",
    "\n",
    "        # Improve performance by trimming to top anchors by score\n",
    "        # and doing the rest on the smaller subset.\n",
    "        pre_nms_limit = tf.minimum(6000, tf.shape(anchors)[1])\n",
    "        ix = tf.nn.top_k(scores, pre_nms_limit, sorted=True, name=\"top_anchors\").indices\n",
    "        ix = tf.cast(ix, dtype=tf.float32)\n",
    "        print(scores)\n",
    "        \n",
    "        scores = tf.map_fn(lambda xy: tf.gather(xy[0], tf.cast(xy[1], dtype=tf.int32)),\n",
    "                           tf.stack([scores, ix], axis=1))\n",
    "        \n",
    "        deltas = tf.map_fn(lambda xy: tf.gather(xy[0], tf.cast(xy[1], dtype=tf.int32)),\n",
    "                           tf.stack([deltas, ix], axis=1))\n",
    "        \n",
    "        pre_nms_anchors = tf.map_fn(lambda ax: tf.gather(ax[0], tf.cast(a[1], dtype=tf.int32)),\n",
    "                                    tf.stack([anchors, ix], axis=1),\n",
    "                                    name=\"pre_nms_anchors\")\n",
    "\n",
    "        # Apply deltas to anchors to get refined anchors.\n",
    "        # [batch, N, (y1, x1, y2, x2)]\n",
    "        boxes = tf.map_fn(lambda xy: apply_box_deltas_graph(xy[0], xy[1]),\n",
    "                          tf.stack([pre_nms_anchors, deltas], axis=1),\n",
    "                          name=\"refined_anchors\")\n",
    "\n",
    "        # Clip to image boundaries. Since we're in normalized coordinates,\n",
    "        # clip to 0..1 range. [batch, N, (y1, x1, y2, x2)]\n",
    "        window = np.array([0, 0, 1, 1], dtype=np.float32)\n",
    "        boxes = tf.map_fn(lambda x: clip_boxes_graph(x, window),\n",
    "                          boxes,\n",
    "                          name=\"refined_anchors_clipped\")\n",
    "\n",
    "        # Filter out small boxes\n",
    "        # According to Xinlei Chen's paper, this reduces detection accuracy\n",
    "        # for small objects, so we're skipping it.\n",
    "\n",
    "        # Non-max suppression\n",
    "        def nms(inputs):\n",
    "            boxes = inputs[0]\n",
    "            scores = inputs[1]\n",
    "            indices = tf.image.non_max_suppression(boxes,\n",
    "                                                   scores,\n",
    "                                                   self.proposal_count,\n",
    "                                                   self.nms_threshold,\n",
    "                                                   name=\"rpn_non_max_suppression\")\n",
    "            proposals = tf.gather(boxes, indices)\n",
    "            \n",
    "            # Pad if needed\n",
    "            padding = tf.maximum(self.proposal_count - tf.shape(proposals)[0], 0)\n",
    "            proposals = tf.pad(proposals, [(0, padding), (0, 0)])\n",
    "            return proposals\n",
    "        \n",
    "        proposals = tf.map_fn(nms, tf.stack([boxes, scores], axis=1))\n",
    "        \n",
    "        return proposals\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.proposal_count, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import keras.layers as KL\n",
    "\n",
    "from maskflow import resnet_graph\n",
    "from maskflow import build_rpn_model\n",
    "from maskflow import ProposalLayer\n",
    "from maskflow.graph import parse_image_meta_graph\n",
    "\n",
    "\n",
    "def maskrcnn_model_fn(features, labels, mode, params):\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        tf.logging.info(\"Mask RCNN Model mode: prediction\")\n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "        tf.logging.info(\"Mask RCNN Model mode: evaluation\")\n",
    "    elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        tf.logging.info(\"Mask RCNN Model mode: training\")\n",
    "        \n",
    "    h = features[\"image_height\"]\n",
    "    w = features[\"image_width\"]\n",
    "    c = features[\"image_channel\"]\n",
    "    \n",
    "    if (params[\"IMAGE_MAX_DIM\"] / 2**6) != int(params[\"IMAGE_MAX_DIM\"] / 2**6):\n",
    "        raise Exception(\"Image size must be dividable by 2 at least 6 times \"\n",
    "                        \"to avoid fractions when downscaling and upscaling.\"\n",
    "                        \"For example, use 256, 320, 384, 448, 512, ... etc. \")\n",
    "    \n",
    "    n_classes = len(params[\"CLASS_NAMES\"])\n",
    "    max_objects = params[\"MAX_OBJECTS\"]\n",
    "\n",
    "    with tf.variable_scope(\"Preprocessing\"):\n",
    "        \n",
    "        # Cast and reshape input images\n",
    "        images = tf.cast(features[\"images\"], tf.float16)\n",
    "        images_shape = tf.cast((-1, h, w, c), tf.int32)\n",
    "        images = tf.reshape(images, images_shape)\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        \n",
    "        # Get class ids and masks if not in predict mod\n",
    "        if mode != tf.estimator.ModeKeys.PREDICT:\n",
    "            class_ids = tf.cast(labels[\"class_ids\"], tf.int32)\n",
    "            class_ids = tf.reshape(class_ids, (batch_size, max_objects))\n",
    "            class_id = class_ids[:, 0]\n",
    "            \n",
    "            masks = tf.cast(labels[\"masks\"], tf.float16)\n",
    "            masks = tf.reshape(masks, tf.stack((batch_size, h, w, -1)))\n",
    "        else:\n",
    "            class_ids = None\n",
    "            masks = None\n",
    "            \n",
    "        # Preprocessing\n",
    "        results = maskflow.preprocessing_graph(images, h, w, c, params)\n",
    "        molded_images, window, image_meta, backbone_shapes, anchors = results\n",
    "\n",
    "    with tf.variable_scope(\"Mask_RCNN_Model\"):\n",
    "        \n",
    "        with tf.variable_scope(\"Resnet_Graph\"):\n",
    "            _, C2, C3, C4, C5 = resnet_graph(molded_images, params[\"BACKBONE\"],\n",
    "                                             stage5=True, train_bn=params[\"TRAIN_BN\"])\n",
    "        \n",
    "        with tf.variable_scope(\"Feature_Pyramid_Network\"):\n",
    "            \n",
    "            top_down_pyramid_size = params[\"TOP_DOWN_PYRAMID_SIZE\"]\n",
    "            \n",
    "            # TODO: add assert to varify feature map sizes match what's in config\n",
    "            P5 = KL.Conv2D(top_down_pyramid_size, (1, 1), name='fpn_c5p5')(C5)\n",
    "            \n",
    "            up = KL.UpSampling2D(size=(2, 2), name=\"fpn_p5upsampled\")(P5)\n",
    "            conv = KL.Conv2D(top_down_pyramid_size, (1, 1), name='fpn_c4p4')(C4)\n",
    "            P4 = KL.Add(name=\"fpn_p4add\")([up, conv])\n",
    "\n",
    "            up = KL.UpSampling2D(size=(2, 2), name=\"fpn_p4upsampled\")(P4)\n",
    "            conv = KL.Conv2D(top_down_pyramid_size, (1, 1), name='fpn_c3p3')(C3)\n",
    "            P3 = KL.Add(name=\"fpn_p3add\")([up, conv])\n",
    "            \n",
    "            up = KL.UpSampling2D(size=(2, 2), name=\"fpn_p3upsampled\")(P3)\n",
    "            conv = KL.Conv2D(top_down_pyramid_size, (1, 1), name='fpn_c2p2')(C2)\n",
    "            P2 = KL.Add(name=\"fpn_p2add\")([up, conv])\n",
    "            \n",
    "            # Attach 3x3 conv to all P layers to get the final feature maps.\n",
    "            P2 = KL.Conv2D(top_down_pyramid_size, (3, 3), padding=\"same\", name=\"fpn_p2\")(P2)\n",
    "            P3 = KL.Conv2D(top_down_pyramid_size, (3, 3), padding=\"same\", name=\"fpn_p3\")(P3)\n",
    "            P4 = KL.Conv2D(top_down_pyramid_size, (3, 3), padding=\"same\", name=\"fpn_p4\")(P4)\n",
    "            P5 = KL.Conv2D(top_down_pyramid_size, (3, 3), padding=\"same\", name=\"fpn_p5\")(P5)\n",
    "            \n",
    "            # P6 is used for the 5th anchor scale in RPN. Generated by\n",
    "            # subsampling from P5 with stride of 2.\n",
    "            P6 = KL.MaxPooling2D(pool_size=(1, 1), strides=2, name=\"fpn_p6\")(P5)\n",
    "\n",
    "            # Note that P6 is used in RPN, but not in the classifier heads.\n",
    "            rpn_feature_maps = [P2, P3, P4, P5, P6]\n",
    "            mrcnn_feature_maps = [P2, P3, P4, P5]        \n",
    "        \n",
    "        with tf.variable_scope(\"Region_Proposal_Network\"):\n",
    "            \n",
    "            rpn = build_rpn_model(params[\"RPN_ANCHOR_STRIDE\"],\n",
    "                                  len(params[\"RPN_ANCHOR_RATIOS\"]),\n",
    "                                  params[\"TOP_DOWN_PYRAMID_SIZE\"])\n",
    "            \n",
    "            # Loop through pyramid layers\n",
    "            layer_outputs = [rpn([p]) for p in rpn_feature_maps]\n",
    "                \n",
    "            # Concatenate layer outputs\n",
    "            # Convert from list of lists of level outputs to list of lists\n",
    "            # of outputs across levels.\n",
    "            # e.g. [[a1, b1, c1], [a2, b2, c2]] => [[a1, a2], [b1, b2], [c1, c2]]\n",
    "            output_names = [\"rpn_class_logits\", \"rpn_class\", \"rpn_bbox\"]\n",
    "            outputs = list(zip(*layer_outputs))\n",
    "            outputs = [KL.Concatenate(axis=1, name=n)(list(o)) for o, n in zip(outputs, output_names)]\n",
    "            \n",
    "            rpn_class_logits, rpn_class, rpn_bbox = outputs\n",
    "            \n",
    "            rpn_class_logits = tf.identity(rpn_class_logits, name=\"rpn_class_logits\")\n",
    "            rpn_class = tf.identity(rpn_class, name=\"rpn_class\")\n",
    "            rpn_bbox = tf.identity(rpn_bbox, name=\"rpn_bbox\")\n",
    "            \n",
    "            with tf.variable_scope(\"Proposal_Layer\"):\n",
    "                # Generate proposals\n",
    "                # Proposals are [batch, N, (y1, x1, y2, x2)] in normalized coordinates\n",
    "                # and zero padded.\n",
    "                proposal_count = tf.cond(tf.equal(mode, tf.estimator.ModeKeys.TRAIN),\n",
    "                                         lambda: params[\"POST_NMS_ROIS_TRAINING\"],\n",
    "                                         lambda: params[\"POST_NMS_ROIS_INFERENCE\"])\n",
    "\n",
    "                proposal_layer = ProposalLayer2(proposal_count,\n",
    "                                               params[\"RPN_NMS_THRESHOLD\"],\n",
    "                                               params[\"RPN_BBOX_STD_DEV\"],\n",
    "                                               name=\"ROI\")\n",
    "                #rpn_rois = proposal_layer([rpn_class, rpn_bbox, anchors])\n",
    "        \n",
    "        \"\"\"\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            \n",
    "            # Class ID mask to mark class IDs supported by the dataset the image\n",
    "            # came from.\n",
    "            active_class_ids = KL.Lambda(\n",
    "                lambda x: parse_image_meta_graph(x)[\"active_class_ids\"]\n",
    "                )(input_image_meta)\n",
    "\n",
    "            if not config.USE_RPN_ROIS:\n",
    "                # Ignore predicted ROIs and use ROIs provided as an input.\n",
    "                input_rois = KL.Input(shape=[config.POST_NMS_ROIS_TRAINING, 4],\n",
    "                                      name=\"input_roi\", dtype=np.int32)\n",
    "                # Normalize coordinates\n",
    "                target_rois = KL.Lambda(lambda x: norm_boxes_graph(\n",
    "                    x, K.shape(input_image)[1:3]))(input_rois)\n",
    "            else:\n",
    "                target_rois = rpn_rois\n",
    "                \n",
    "        else:\n",
    "            pass\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "\n",
    "        predictions = {\"rpn_class_logits\": rpn_class_logits}\n",
    "\n",
    "    # 1. Prediction mode\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predict_output = {\"predict\": tf.estimator.export.PredictOutput(predictions)}\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions,\n",
    "                                          export_outputs=predict_output)\n",
    "            \n",
    "    with tf.variable_scope(\"Losses\"):\n",
    "        rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=\"rpn_class_loss\")(\n",
    "                [input_rpn_match, rpn_class_logits])\n",
    "\n",
    "        # Calculate the accuracy between the true labels, and our predictions\n",
    "        #accuracy = tf.metrics.accuracy(labels=class_id, predictions=predictions[\"class_ids\"])\n",
    "\n",
    "    # 2. Evaluation mode\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss)#, eval_metric_ops={\"accuracy\": accuracy})\n",
    "\n",
    "    # 3. Training mode\n",
    "    with tf.variable_scope(\"Training\"):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=params[\"learning_rate\"])\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        \n",
    "    tf.summary.image(\"input_images\", images)\n",
    "    tf.summary.image(\"input_masks\", tf.expand_dims(masks[:, :, :, 0], axis=-1))\n",
    "\n",
    "    tf.summary.image(\"C2\", C2)\n",
    "    tf.summary.image(\"C3\", C3)\n",
    "    tf.summary.image(\"C4\", C4)\n",
    "    tf.summary.image(\"C5\", C5)\n",
    "    \n",
    "    tf.summary.image(\"P2\", P2)\n",
    "    tf.summary.image(\"P3\", P3)\n",
    "    tf.summary.image(\"P4\", P4)\n",
    "    tf.summary.image(\"P5\", P5)\n",
    "    tf.summary.image(\"P6\", P6)\n",
    "        \n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/home/hadim/.data/Neural_Network/Maskflow/Shapes/Logs/Shapes Detector', '_tf_random_seed': None, '_save_summary_steps': 150, '_save_checkpoints_steps': 150, '_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 150, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f06989fee10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Training settings\n",
    "n_epochs = 200\n",
    "batch_size = 8\n",
    "learning_rate = 0.0001\n",
    "restore_checkpoint = False\n",
    "\n",
    "train_size = len(image_paths_train)\n",
    "eval_size = len(image_paths_eval)\n",
    "steps_per_epoch = train_size // batch_size + 100\n",
    "eval_steps = eval_size // batch_size\n",
    "total_training_steps = steps_per_epoch * n_epochs\n",
    "\n",
    "# Construct the estimator\n",
    "run_config = tf.estimator.RunConfig(save_summary_steps=steps_per_epoch,\n",
    "                                    save_checkpoints_steps=steps_per_epoch,\n",
    "                                    log_step_count_steps=steps_per_epoch)\n",
    "\n",
    "params = config.copy()\n",
    "params[\"learning_rate\"] = learning_rate\n",
    "estimator = tf.estimator.Estimator(model_fn=maskrcnn_model_fn, model_dir=model_dir, params=params, config=run_config)\n",
    "\n",
    "# Setup the dataset\n",
    "train_dataset_input = input_fn(image_paths_train, mask_paths_train, class_ids_paths_train,\n",
    "                               batch_size, config[\"MAX_OBJECTS\"], shuffle=True)\n",
    "\n",
    "eval_dataset_input = input_fn(image_paths_eval, mask_paths_eval, class_ids_paths_eval,\n",
    "                              batch_size, config[\"MAX_OBJECTS\"], shuffle=False)\n",
    "\n",
    "# Setup training and evaluation\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_dataset_input, max_steps=total_training_steps)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=eval_dataset_input, steps=eval_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:n_epochs: 200\n",
      "INFO:tensorflow:batch_size: 8\n",
      "INFO:tensorflow:learning_rate: 0.0001\n",
      "INFO:tensorflow:train_size: 400\n",
      "INFO:tensorflow:eval_size: 100\n",
      "INFO:tensorflow:steps_per_epoch: 150\n",
      "INFO:tensorflow:eval_steps: 12\n",
      "INFO:tensorflow:total_training_steps: 30000\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Mask RCNN Model mode: training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_rpn_match' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-26bbf4104d66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/local/conda/envs/nn/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m(estimator, train_spec, eval_spec)\u001b[0m\n\u001b[1;32m    437\u001b[0m         '(with task id 0).  Given task id {}'.format(config.task_id))\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m   \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/conda/envs/nn/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m         config.task_type != run_config_lib.TaskType.EVALUATOR):\n\u001b[1;32m    517\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running training and evaluation locally (non-distributed).'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/conda/envs/nn/lib/python3.6/site-packages/tensorflow/python/estimator/training.py\u001b[0m in \u001b[0;36mrun_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    648\u001b[0m           \u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m           \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m           hooks=train_hooks)\n\u001b[0m\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m       \u001b[0;31m# Final export signal: For any eval result with global_step >= train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/conda/envs/nn/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/conda/envs/nn/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    841\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/conda/envs/nn/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m--> 856\u001b[0;31m           features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m    857\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[1;32m    858\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/local/conda/envs/nn/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 831\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    832\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-3cd89ef94bb7>\u001b[0m in \u001b[0;36mmaskrcnn_model_fn\u001b[0;34m(features, labels, mode, params)\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Losses\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         rpn_class_loss = KL.Lambda(lambda x: rpn_class_loss_graph(*x), name=\"rpn_class_loss\")(\n\u001b[0;32m--> 166\u001b[0;31m                 [input_rpn_match, rpn_class_logits])\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# Calculate the accuracy between the true labels, and our predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_rpn_match' is not defined"
     ]
    }
   ],
   "source": [
    "# Run training and evaluation\n",
    "tf.logging.info(f\"n_epochs: {n_epochs}\")\n",
    "tf.logging.info(f\"batch_size: {batch_size}\")\n",
    "tf.logging.info(f\"learning_rate: {learning_rate}\")\n",
    "tf.logging.info(f\"train_size: {train_size}\")\n",
    "tf.logging.info(f\"eval_size: {eval_size}\")\n",
    "tf.logging.info(f\"steps_per_epoch: {steps_per_epoch}\")\n",
    "tf.logging.info(f\"eval_steps: {eval_steps}\")\n",
    "tf.logging.info(f\"total_training_steps: {total_training_steps}\")\n",
    "\n",
    "if not restore_checkpoint:\n",
    "    shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "for _ in range(n_epochs):\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Mask RCNN Model mode: prediction\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/hadim/.data/Neural_Network/Maskflow/Shapes/Logs/Shapes Detector/model.ckpt-50\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "def serving_input_receiver_fn():\n",
    "    inputs = {}\n",
    "    inputs[\"images\"] = tf.placeholder(tf.float32, shape=(None, None, None, None))\n",
    "    inputs[\"original_image_height\"] = tf.placeholder(tf.int32, shape=())\n",
    "    inputs[\"original_image_width\"] = tf.placeholder(tf.int32, shape=())\n",
    "    inputs[\"original_image_channel\"] = tf.placeholder(tf.int32, shape=())\n",
    "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n",
    "\n",
    "predictor = tf.contrib.predictor.from_estimator(estimator, serving_input_receiver_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True class: circle\n",
      "Predicted class: circle\n",
      "['BG: 0.00', 'square: 0.00', 'circle: 1.00', 'triangle: 0.00']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#data_id = 20\n",
    "data_id += 1\n",
    "\n",
    "image_path = image_paths[data_id]\n",
    "mask_path = mask_paths[data_id]\n",
    "class_ids_path = class_ids_paths[data_id]\n",
    "\n",
    "# Load data\n",
    "image, mask, class_ids = maskflow.load_data(image_path, mask_path, class_ids_path, config[\"MAX_OBJECTS\"])\n",
    "\n",
    "# Build inputs\n",
    "inputs = {}\n",
    "inputs[\"images\"] = np.expand_dims(image, axis=0)\n",
    "inputs[\"original_image_height\"] = image.shape[0]\n",
    "inputs[\"original_image_width\"] = image.shape[1]\n",
    "inputs[\"original_image_channel\"] = image.shape[2]\n",
    "\n",
    "# Run prediction\n",
    "results = predictor(inputs)\n",
    "\n",
    "# Display results\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2)\n",
    "axs[0].imshow(image)\n",
    "axs[1].imshow(mask[:, :, 0])\n",
    "\n",
    "true_class = config[\"CLASS_NAMES\"][int(class_ids[0])]\n",
    "predicted_class = config[\"CLASS_NAMES\"][int(results[\"class_ids\"][0])]\n",
    "\n",
    "print(f\"True class: {true_class}\")\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print([f\"{config['CLASS_NAMES'][i]}: {p:.2f}\" for i, p in enumerate(results[\"probabilities\"][0])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export TensorFlow Model to SavedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Mask RCNN Model mode: prediction\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict', 'serving_default']\n",
      "INFO:tensorflow:Restoring parameters from /home/hadim/.data/Neural_Network/Maskflow/Shapes/Logs/Shapes Detector/model.ckpt-600\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"/home/hadim/.data/Neural_Network/Maskflow/Shapes/SavedModels/Shapes Detector/temp-b'1528653573'/saved_model.pb\"\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(saved_model_dir, ignore_errors=True)\n",
    "\n",
    "assets_extra = {\"config.yml\": str(root_dir / \"config.yml\")}\n",
    "tmp_exported_dir = estimator.export_savedmodel(export_dir_base=str(saved_model_dir), serving_input_receiver_fn=serving_input_receiver_fn, assets_extra=assets_extra)\n",
    "\n",
    "import time; time.sleep(2)\n",
    "\n",
    "# Move all the files to the parent folder\n",
    "for fpath in Path(tmp_exported_dir.decode(\"utf-8\")).iterdir():\n",
    "    shutil.move(str(fpath), saved_model_dir)\n",
    "shutil.rmtree(tmp_exported_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package SavedModel to a ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_model_path = root_dir / \"SavedModels\" / (model_basename.replace(\" \", \"_\") + \".zip\")\n",
    "\n",
    "def zip_dir(root_dir, path_dir, z):\n",
    "    for fpath in path_dir.iterdir():\n",
    "        if fpath.is_file():\n",
    "            zip_path = fpath.relative_to(root_dir)\n",
    "            z.write(fpath, arcname=zip_path)\n",
    "        elif fpath.is_dir():\n",
    "            zip_dir(root_dir, fpath, z)\n",
    "\n",
    "with zipfile.ZipFile(zip_model_path, \"w\") as z:\n",
    "    zip_dir(saved_model_dir, saved_model_dir, z)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nn]",
   "language": "python",
   "name": "conda-env-nn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
